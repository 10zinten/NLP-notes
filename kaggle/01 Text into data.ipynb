{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "1. **Tokenizing**: Splitting the text into units of language.  `Use tokenizer`\n",
    "    1. Can use words (works for language like Enligsh), but it's harder to segment languages w/o a character to indicate segmentation (like Tibetan, chinese)\n",
    "    2. Can use morphemes, which workds well for morpholigical complex langauge like German and Inuktiut.\n",
    "2. **Stemming**: Figuring out that different forms of the same word are root. At word level not morpheme level. `Use Stemmer`\n",
    "\n",
    "    - Examples: (un)(structur)(ed) \n",
    "\n",
    "    - All these words shared a root:\n",
    "        * structuring\n",
    "        * structured\n",
    "        * structurable\n",
    "        * structure\n",
    "        \n",
    "3. **Parsing** (for data anaylsis probability not needed)\n",
    "    - Figure out & represent the relationship between words)\n",
    "\n",
    "# Misspelled words -> Fuzzy matching with Stemming\n",
    "Challenges:\n",
    "- They may be unique: **Zipf's law** -> Most words in the dictionary would be very uncommon is the text and very few words like a, the, of, ect. function words would be bulk in any given text. \n",
    "- They may not be very like the intended word.\n",
    "- [W-NUT](http://noisy-text.github.io/2018/) papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
